---
title: 'ViLBERT, a model for learning joint representations of image and text'
date: 2021-05-05
permalink: /posts/2021/05/blog-post-1/
tags:
  - Multi-modal machine learning
  - Deep Learning
  - Computer Vision
  - Natural Language Processing
---

Many real-world applications don't involve only one data modality. Web pages, for example, contain text, images, videos, etc. Restricting oneself to using only one modality would involve losing all the information contained in the others. Multi-modal machine learning aims to build models that can process and relate information from multiple modalities. In this tutorial, we focus on two main modalities: written text as linguistic and image as visual signals.
Joint image-text representation is the bedrock for many Vision-and-Language tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. This enables a wide range of applications, such as visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval.

This article was published in Medium, more information can be found here <a href="[ https://naserian-elahe.medium.com/deep-embedding-and-clustering-an-step-by-step-python-implementation-bd2c9d51c80f/](https://naserian-elahe.medium.com/vilbert-a-model-for-learning-joint-representations-of-image-content-and-natural-language-47f56a313a79)" target="_blank" rel="noopener noreferrer">ViLBERT/a>




------
